{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIvXsLg7SBF0"
   },
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51DOdmUkuZmX"
   },
   "source": [
    "In this section we import the chosen five different data sets (finance, news, movies, twitter and reddit). Each data frame will haev the same formatting, only retaining three columns:\n",
    "- text: text input\n",
    "- ground_truth: sentiment label (positive, negative, neutral)\n",
    "- topic: indicates from which dataset the observations comes from (finance, news, movies, twitter, reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6x3ql8H_NSa"
   },
   "outputs": [],
   "source": [
    "#! pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfNuJrzrTLlD"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGBUPaPmTypH",
    "outputId": "be898feb-249d-481f-f27a-4d2c59b3adaa"
   },
   "outputs": [],
   "source": [
    "# finance data\n",
    "\n",
    "path = kagglehub.dataset_download(\"sbhatti/financial-sentiment-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J85K36a8VVER"
   },
   "outputs": [],
   "source": [
    "csv_file = os.path.join(path, \"data.csv\")\n",
    "\n",
    "df_finance = pd.read_csv(csv_file, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Pm4JzERmAhI6",
    "outputId": "e5c68e5b-fc4b-4e2d-fe52-6cb1a03c99e1"
   },
   "outputs": [],
   "source": [
    "df_finance.rename(columns={df_finance.columns[0]: \"text\"}, inplace=True)\n",
    "df_finance.rename(columns={df_finance.columns[1]: \"ground_truth\"}, inplace=True)\n",
    "df_finance[\"topic\"] = \"finance\"\n",
    "\n",
    "df_finance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1Y3xNUXYRC9",
    "outputId": "3e7a3415-c891-4363-d8db-424a50b14a54"
   },
   "outputs": [],
   "source": [
    "# news data\n",
    "\n",
    "path = kagglehub.dataset_download(\"cashbowman/sentiment-labeled-headlines\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xGaXav5ysdn"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(path, 'Sentiments', 'guardian_sentiment.csv')\n",
    "\n",
    "df_news = pd.read_csv(path, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GFk6jVryzKJK",
    "outputId": "63dfb0b0-290d-4507-d077-4ba44ceec154"
   },
   "outputs": [],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "F32WN7jwcp6q",
    "outputId": "c84b6e29-b074-4c0d-8575-5437f830d571"
   },
   "outputs": [],
   "source": [
    "df_news.rename(columns={df_news.columns[2]: \"text\"}, inplace=True)\n",
    "df_news.rename(columns={df_news.columns[3]: \"ground_truth\"}, inplace=True)\n",
    "df_news[\"topic\"] = \"news\"\n",
    "df_news = df_news[[\"text\", \"ground_truth\", \"topic\"]]\n",
    "\n",
    "df_news.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-bG6sgp3lF5"
   },
   "source": [
    "In this data set, the default sentiment classification is a five-step scale. We will map these to have very negative (1) as negative, neutral (3) as neutral and very positive (5) as positive. We will drop observations that are rated 2 (negative) and 4 (positive) to only have clear cases and stronger sentiment intensity in our observaitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6CYeuwl4HoI"
   },
   "outputs": [],
   "source": [
    "mapping = {1: \"negative\", 3: \"neutral\", 5: \"positive\"}\n",
    "\n",
    "# Apply mapping to the \"ground_truth\" column\n",
    "df_news[\"ground_truth\"] = df_news[\"ground_truth\"].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "7xE9buhT4Ii9",
    "outputId": "5b7588b5-0378-4570-d168-e245c2b6d4aa"
   },
   "outputs": [],
   "source": [
    "df_news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--oGuyhCbRkW",
    "outputId": "4926bbea-c5e2-4a26-e067-00a9931a64a6"
   },
   "outputs": [],
   "source": [
    "# reddit data\n",
    "\n",
    "path = kagglehub.dataset_download(\"cosmos98/twitter-and-reddit-sentimental-analysis-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5sDso_ymGYS"
   },
   "outputs": [],
   "source": [
    "csv_file = os.path.join(path, \"Reddit_Data.csv\")\n",
    "\n",
    "df_reddit = pd.read_csv(csv_file, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Mdq1QfmUj6"
   },
   "outputs": [],
   "source": [
    "df_reddit[\"topic\"] = \"reddit\"\n",
    "df_reddit.rename(columns={df_reddit.columns[0]: \"text\"}, inplace=True)\n",
    "df_reddit.rename(columns={df_reddit.columns[1]: \"ground_truth\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-5WsW8a5lCJ"
   },
   "source": [
    "The dataset labels the sentiment as -1 (negative), 0 (neutral) and 1 (positive). For conformity with other data sets, this is mapped to text labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGv9-2POnHlP"
   },
   "outputs": [],
   "source": [
    "# Define the mapping\n",
    "mapping = {-1: \"negative\", 0: \"neutral\", 1: \"positive\"}\n",
    "\n",
    "# Apply mapping to the \"ground_truth\" column\n",
    "df_reddit[\"ground_truth\"] = df_reddit[\"ground_truth\"].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5JRKq-k6nPhS",
    "outputId": "e37416e4-b1ae-4980-d8c2-eab2b428a53b"
   },
   "outputs": [],
   "source": [
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL7feIUJoPHu"
   },
   "outputs": [],
   "source": [
    "# twitter data\n",
    "\n",
    "csv_file = os.path.join(path, \"Twitter_Data.csv\")\n",
    "\n",
    "#df = pd.read_csv(csv_file)\n",
    "df_twitter = pd.read_csv(csv_file, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4emCVt4rogLC"
   },
   "outputs": [],
   "source": [
    "df_twitter[\"topic\"] = \"twitter\"\n",
    "df_twitter.rename(columns={df_twitter.columns[0]: \"text\"}, inplace=True)\n",
    "df_twitter.rename(columns={df_twitter.columns[1]: \"ground_truth\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoJ0Psf4pgOJ"
   },
   "outputs": [],
   "source": [
    "df_twitter[\"ground_truth\"] = df_twitter[\"ground_truth\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Qeza1kb2puCp",
    "outputId": "bfb0dc4f-539e-4154-b612-e082facf570f"
   },
   "outputs": [],
   "source": [
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PZytjs9qJ6i",
    "outputId": "c0f227e5-df7f-4792-fad2-77a5639ddc49"
   },
   "outputs": [],
   "source": [
    "# movie reviews\n",
    "path = kagglehub.dataset_download(\"yacharki/movie-review-sentiment-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05nb-rVhsM9r",
    "outputId": "05498b61-ef75-4167-a5cd-0c6f81e1b166"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(path, 'Movie Reviews Sentences for Sentiment Analysis NLP')\n",
    "os.listdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k94dSaC1qam0"
   },
   "outputs": [],
   "source": [
    "csv_file = os.path.join(path, \"test.csv\")\n",
    "\n",
    "\n",
    "df_movie_1 = pd.read_csv(csv_file, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvU9ltLs0TLw"
   },
   "outputs": [],
   "source": [
    "csv_file = os.path.join(path, \"train.csv\")\n",
    "\n",
    "\n",
    "df_movie_2 = pd.read_csv(csv_file, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buxBQbUl1Eu6"
   },
   "outputs": [],
   "source": [
    "df_movie = pd.concat([df_movie_1, df_movie_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcFyecgI2CPn"
   },
   "outputs": [],
   "source": [
    "df_movie[\"topic\"] = \"movie\"\n",
    "df_movie.rename(columns={df_movie.columns[0]: \"ground_truth\"}, inplace=True)\n",
    "df_movie.rename(columns={df_movie.columns[1]: \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCJ8WYMu2f8-",
    "outputId": "c03dc688-b267-4457-fa83-68151ecc5206"
   },
   "outputs": [],
   "source": [
    "df_movie[\"ground_truth\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60wf8ojL56S8"
   },
   "source": [
    "This data also uses a 5-step scale, where 0 (very negative) will be converted to negative, 2 (neutral) will become neutral, 4 (very positive) will be positive. 1 (negative) and 3 (positive) will be droppped to retain only texts with stronger sentiment intensity. This is the same logic as in case of the news data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IK1pWcu17ZO"
   },
   "outputs": [],
   "source": [
    "# Define the mapping\n",
    "mapping = {0: \"negative\", 2: \"neutral\", 4: \"positive\"}\n",
    "\n",
    "# Apply mapping to the \"ground_truth\" column\n",
    "df_movie[\"ground_truth\"] = df_movie[\"ground_truth\"].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "L-jNJFG2674n",
    "outputId": "5fa757fa-35d9-4974-e12e-f53e8b12f819"
   },
   "outputs": [],
   "source": [
    "df_movie.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkiSErBpqVfa"
   },
   "source": [
    "# Dataset comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-shyAkDlTbQK"
   },
   "source": [
    "We imported the datasets that will be used. They are:\n",
    "- df_finance\n",
    "- df_news\n",
    "- df_reddit\n",
    "- df_twitter\n",
    "- df_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dtqvrxzpq8Y5"
   },
   "outputs": [],
   "source": [
    "# creating one big dataset\n",
    "\n",
    "df_all = pd.concat([df_finance, df_news, df_reddit, df_twitter, df_movie], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "othdF5M2SD-7",
    "outputId": "a5ff7f01-92cf-4eac-8ec8-01c2c9313cea"
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Ydunog8qvp"
   },
   "source": [
    "Before proceeding we will drop duplicates (if any) and single word text inputs. Single word observations, esspecially for Naive Bayes model, are hard to classifiy if the word is unseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_ahDeg69A1V"
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop_duplicates(subset='text', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1bYik8VSS91",
    "outputId": "e7a89a98-3c8e-4967-b980-afb19b14b7a2"
   },
   "outputs": [],
   "source": [
    "df_all.shape # 1,323 duplicates have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF8KpAEc9iYw"
   },
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['text'].str.split().str.len() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvyE-OT29zKa",
    "outputId": "ba7943fe-9db7-4e86-a18e-e8f3d7a63dd3"
   },
   "outputs": [],
   "source": [
    "df_all.shape  # 45,060 single-word and double-word inputs have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nvm-C3ZnZOiT"
   },
   "outputs": [],
   "source": [
    "df_all = df_all.dropna(subset=['ground_truth'])\n",
    "df_all = df_all[df_all['ground_truth'] != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aw4wooFZX_f",
    "outputId": "6a5abdac-5367-44a5-d9d5-06c53b228682"
   },
   "outputs": [],
   "source": [
    "df_all.shape # 37,638 observations dropped with missing sentiment label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYyf3NU_P_a4"
   },
   "source": [
    "The length of the text inputs is also investigated. It is optimal to stay within a defined range of word counts to obtain homogenous text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOkn-3vSQcvk"
   },
   "outputs": [],
   "source": [
    "def input_lengths (data):\n",
    "  data['word_count'] = data['text'].str.split().str.len()\n",
    "  topics = data['topic'].unique()\n",
    "  df_length = pd.DataFrame(columns=['topic', 'average_words', 'median_words', 'shortest', 'longest'])\n",
    "  for topic in topics:\n",
    "    topic_df = data[data['topic'] == topic]\n",
    "    average_words = topic_df['word_count'].mean()\n",
    "    median_words = topic_df['word_count'].median()\n",
    "    shortest = topic_df['word_count'].min()\n",
    "    longest = topic_df['word_count'].max()\n",
    "    df_length.loc[len(df_length)] = [topic, average_words, median_words, shortest, longest]\n",
    "  return df_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNQFU1TvRTXi",
    "outputId": "021c27db-88fb-4dd9-fc2b-4243238035c3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dq5vd5u4QtM7",
    "outputId": "49a77196-6718-429b-b15a-cc1e1aeecf20"
   },
   "outputs": [],
   "source": [
    "word_counts = input_lengths(df_all)\n",
    "word_counts.to_csv(\"/content/drive/My Drive/BA THESIS/analysis/raw_word_counts.csv\")\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "9JzaSwRNQtjU",
    "outputId": "e22e2041-2e1e-4372-f2c2-7960269b5386"
   },
   "outputs": [],
   "source": [
    "# Boxplot of word count\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x = 'topic', y = 'word_count', data = df_all)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Word Count by Topic')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5VnlfwUR-WM"
   },
   "source": [
    "The median inut lengths are between 4 and 20 for each topic. To obtain more convergent distribution, all observations with longer than 50 words will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCDdfMElQt_P",
    "outputId": "a4a69292-683d-4471-a312-a9798a66c410"
   },
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['word_count'] <= 50] # 5,312 observations were removed\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "UBNfZxqHTXYe",
    "outputId": "c528eac4-b6b2-4dff-f3c4-c8ae21e21742"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x = 'topic', y = 'word_count', data = df_all)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Word Count by Topic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/drive/My Drive/BA THESIS/analysis/raw_word_count_distribution_box.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "XeWRziOEXkK9",
    "outputId": "919f2b87-8de0-4ac2-9f8d-2bcdf63690e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for topic in df_all['topic'].unique():\n",
    "    topic_data = df_all[df_all['topic'] == topic]['word_count']\n",
    "    values, counts = topic_data.value_counts().sort_index().values, topic_data.value_counts().sort_index().index\n",
    "    relative_freq = values/values.sum()\n",
    "    plt.plot(counts, relative_freq, label=topic, marker=None)\n",
    "\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Relative Frequency\")\n",
    "plt.title(\"Word Count Distribution by Topic (Relative Frequency)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/content/drive/My Drive/BA THESIS/analysis/raw_word_count_distribution_line.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbcPrkd0Y1VQ",
    "outputId": "b180f069-21b1-47b0-e547-c3214b0328af"
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Xz_nAOZ5S58G",
    "outputId": "b3a16728-a1a2-4890-90df-eaac838a7255"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_all[\"ground_truth\"], df_all[\"topic\"], margins=True)\n",
    "table.columns.name = None\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0DJ8b4fa9Bk"
   },
   "outputs": [],
   "source": [
    "table.to_csv(\"/content/drive/My Drive/BA THESIS/analysis/raw_table_sentiment_topic_distribution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxpXh4sITWrK"
   },
   "source": [
    "Accross all topics except for news, the \"negative\" class is the least represented. In all these categories we will resample the \"neutral\" and \"positive\" category to have the categories balanced within topics. In case of the news data, we resample it to the neutral category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ykj1Tg8gDIP"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpb_C6yQg9Lk",
    "outputId": "5ce3ed42-bb54-439b-82ef-c9fbce1f4ea4"
   },
   "outputs": [],
   "source": [
    "def balance_ground_truth(group):\n",
    "\n",
    "    # counts of each class in this group\n",
    "    class_counts = group[\"ground_truth\"].value_counts()\n",
    "\n",
    "    # minimum count among the classes\n",
    "    min_count = class_counts.min()\n",
    "\n",
    "    # Resample each class to the minimum count\n",
    "    balanced_dfs = []\n",
    "    for label in class_counts.index:\n",
    "        df_class = group[group[\"ground_truth\"] == label]\n",
    "\n",
    "        # Only resample if the class has more than min_count\n",
    "        if len(df_class) > min_count:\n",
    "            df_class = resample(df_class, replace=False, n_samples=min_count, random_state=42)\n",
    "\n",
    "        balanced_dfs.append(df_class)\n",
    "\n",
    "    # Combine all balanced class DataFrames\n",
    "    return pd.concat(balanced_dfs).reset_index(drop=True)\n",
    "\n",
    "# Apply the resampling function to each topic separately\n",
    "df_resampled = df_all.groupby(\"topic\", group_keys=False).apply(balance_ground_truth)\n",
    "\n",
    "print(df_resampled.groupby([\"topic\", \"ground_truth\"]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fLszaGSGs2g"
   },
   "source": [
    "Now the sentiments are balanced but the topics still need to be balanced. To not loose to many observations we will sample down each topic that has more than 2,000 observations per sentiment to 2,000. This concerns reddit, twitter and movies topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkr8FJXqrK1U",
    "outputId": "3a0b731e-c480-43ec-d7c5-fdf63d242066"
   },
   "outputs": [],
   "source": [
    "# sample down to 2,000 each label in movie, reddit and twitter\n",
    "\n",
    "sample_topics = ['movie', 'reddit', 'twitter']\n",
    "\n",
    "# Split the data into two parts:\n",
    "# 1. Rows where the topic is one of the sample_topics\n",
    "df_to_sample = df_resampled[df_resampled['topic'].isin(sample_topics)]\n",
    "# 2. Rows where the topic is not one of the sample_topics (to be retained as-is)\n",
    "df_remaining = df_resampled[~df_resampled['topic'].isin(sample_topics)]\n",
    "\n",
    "# For each ground_truth category within the sample topics, randomly sample 2000 rows.\n",
    "sampled_df = df_to_sample.groupby(['ground_truth', 'topic'], group_keys=False).apply(lambda x: x.sample(n=2000, random_state=222))\n",
    "\n",
    "# Combine the sampled subset with the rest of the data.\n",
    "df_resampled = pd.concat([sampled_df, df_remaining]).reset_index(drop=True)\n",
    "\n",
    "# Optional: Verify counts\n",
    "print(df_resampled.groupby(['topic', 'ground_truth']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOMlMjGshb60"
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "df_balanced = df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSjejiLgbUh4"
   },
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.drop(columns=['word_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QMB3HRUWhpSS",
    "outputId": "29ade5c5-9960-445a-f992-e38bfd787531"
   },
   "outputs": [],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agNXeyhpa9Kx"
   },
   "outputs": [],
   "source": [
    "df_balanced.to_csv(\"/content/drive/My Drive/BA THESIS/data/df_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuwAdirnkb4F"
   },
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOmTqDJGnN54"
   },
   "source": [
    "Naive Bayes and Bert require different processing. For Naive Bayes, first negations are treated, tehn stopwords are removed and finally all special characters and numbers are removed. For BERT only special characters and numbers are removed but stop words and negations do not need to eb treated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP1-Fd14CAqY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xp9qunEuIFl"
   },
   "outputs": [],
   "source": [
    "df_nb = pd.read_csv(\"/content/drive/My Drive/BA THESIS/data/df_balanced.csv\")\n",
    "df_bert = pd.read_csv(\"/content/drive/My Drive/BA THESIS/data/df_balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9KNkV4VvCT1G",
    "outputId": "7744935f-72b6-4cd8-826a-49c6009abd85"
   },
   "outputs": [],
   "source": [
    "df_nb.loc[109:113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "P9W94MAHEazp",
    "outputId": "67d081cb-1bb3-462c-e3bd-27354e0dd10c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_nb.loc[109:113]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6JINEqVi3qV"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ct5hEhFbkp9Y",
    "outputId": "a7e2fbf7-64b3-4f6a-c18c-ff9bcaff5ab2"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ-jo93Yko-f"
   },
   "source": [
    "First negations are processed. In a sentence \"not good\" will become NOT_good or \"I wasn't happy\" will be NOT_happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uteWuiH0iONA"
   },
   "outputs": [],
   "source": [
    "def preprocess_negation(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    # Tokenize by whitespace only\n",
    "    words = text.split()\n",
    "\n",
    "    processed_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "        if word in [\"no\", \"not\", \"nor\", \"wasn't\", \"weren't\", \"wouldn't\", \"won't\", \"can't\", \"couldn't\",\n",
    "                   \"hasn't\", \"haven't\", \"hadn't\", \"isn't\", \"aren't\", \"shouldn't\", \"shan't\", \"needn't\",\n",
    "                   \"mightn't\", \"don't\", \"doesn't\"]:\n",
    "            if i + 1 < len(words):\n",
    "                processed_words.append(\"NOT_\" + words[i+1])\n",
    "                i += 2  # Skip the next word as it's already processed\n",
    "            else:\n",
    "                processed_words.append(word) # Handle the case where negation word is the last word\n",
    "                i += 1\n",
    "        elif word in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "            i += 1\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4-k6pbUk3L-"
   },
   "outputs": [],
   "source": [
    "df_nb['text'] = df_nb['text'].apply(preprocess_negation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4InxmRPmhVI"
   },
   "source": [
    "Then special characers and numbers are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SieDlgckmgfE"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters_and_numbers(text):\n",
    "    # Keep only letters, spaces, and \"NOT_\"\n",
    "    text = re.sub(r'[^a-zA-Z\\sNOT_]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tRoQ2o4mzCs"
   },
   "outputs": [],
   "source": [
    "df_nb['text'] = df_nb['text'].apply(remove_special_characters_and_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzbDoXnouX1U"
   },
   "outputs": [],
   "source": [
    "df_bert['text'] = df_bert['text'].apply(remove_special_characters_and_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gfn_8QopAvgj",
    "outputId": "e8f79567-f158-45d2-8cea-d9e8801df981"
   },
   "outputs": [],
   "source": [
    "df_nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OH-Ws4QIvY-a",
    "outputId": "eec3dc7f-31b5-4f03-a709-6469f93a9469"
   },
   "outputs": [],
   "source": [
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8soQsepIk1t"
   },
   "source": [
    "Stop words, numbers and special characters have been removed. Now we inspect if any text input remains empty after this processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjn44PkMzjGI",
    "outputId": "4fc65e46-83fb-4d20-d9f2-a8958b969bbf"
   },
   "outputs": [],
   "source": [
    "df_nb.shape == df_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6D_2TspJM47"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xc6NIdBwIgxN",
    "outputId": "f9b83b4a-1938-407f-8a44-cccec6c1c601"
   },
   "outputs": [],
   "source": [
    "df_nb[\"text\"] = df_nb[\"text\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_bert[\"text\"] = df_bert[\"text\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "print(df_nb.isna().sum(), df_bert.isna().sum())  # See NaN counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIOovrkcJg-L",
    "outputId": "2a5b8841-43b4-437b-fe4c-1992b703c602"
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^\\s*nan\\s*$', flags=re.IGNORECASE)\n",
    "\n",
    "df_nb[\"text\"] = df_nb[\"text\"].replace(pattern, np.nan, regex=True)\n",
    "df_bert[\"text\"] = df_bert[\"text\"].replace(pattern, np.nan, regex=True)\n",
    "\n",
    "print(df_nb.isna().sum(), df_bert.isna().sum())  # See NaN counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwCt2LPLz1xI"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with NA values in 'text' column of df_nb\n",
    "df_nb.dropna(inplace=True)\n",
    "\n",
    "# Get the index of rows that were dropped from df_nb\n",
    "index_to_drop = df_bert.index.difference(df_nb.index)\n",
    "\n",
    "# Drop the same observations from df_bert\n",
    "df_bert.drop(index_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzvQbh0jcWrZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reset index for df_nb and df_bert\n",
    "df_nb = df_nb.reset_index(drop=True)\n",
    "df_bert = df_bert.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmlhDg77KHO8",
    "outputId": "22a82ddd-b2e9-42db-f1ca-c4aca3041832"
   },
   "outputs": [],
   "source": [
    "df_nb.shape == df_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M81etti0ntrf"
   },
   "source": [
    "After processing, some text inputs remained empty. These were also removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2xtTsEpfbqRA",
    "outputId": "d7223eed-d03e-4baf-fe7c-6404801eda81"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_bert[\"ground_truth\"], df_bert[\"topic\"], margins=True)\n",
    "table.columns.name = None\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkOGmVyDbwNM"
   },
   "outputs": [],
   "source": [
    "table.to_csv(\"/content/drive/My Drive/BA THESIS/analysis/raw_table_sentiment_topic_distribution_after_processing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "Gf8kYTFC0zrn",
    "outputId": "cdcf7c9a-c658-4137-a284-9478854d5d5a"
   },
   "outputs": [],
   "source": [
    "df_bert.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrYgUK07Kkq2"
   },
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16ZGMPOZKy-S"
   },
   "source": [
    "Additionnal processing is required for the Naive Bayes model. Both stemming and lemmatizing will be tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD5vNfipsN03"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNHala5humgm",
    "outputId": "7f7b3690-bb0a-4526-8f43-9975851f5c83"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp2SEMwpsuaU"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr-U3RjTwwyX"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character for lemmatization\"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    tag = pos_tag([word])[0][1][0].upper()  # Get the POS tag and first letter\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5b9E3GQsyco"
   },
   "outputs": [],
   "source": [
    "df_NB_lem = df_nb.copy()\n",
    "\n",
    "def lemm_text(text):\n",
    "\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words if word not in stop_words]\n",
    "    # Reconstruct the text\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df_NB_lem[\"text\"] = df_NB_lem[\"text\"].astype(str).apply(lemm_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-IB-ABrMLNy"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZYzTRvg218B"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df_NB_stem = df_nb.copy()\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def stem_text(text):\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and stem words\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    # Reconstruct the text\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply stemming to the text column\n",
    "df_NB_stem[\"text\"] = df_NB_stem[\"text\"].astype(str).apply(stem_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "lJDLOYlc11B1",
    "outputId": "c3ca51e0-769c-492b-eec5-acd9e21fac86"
   },
   "outputs": [],
   "source": [
    "df_NB_lem.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "pICcR8JP1-YO",
    "outputId": "eba5e439-7467-4a0e-8b55-0e9e351ec6ca"
   },
   "outputs": [],
   "source": [
    "df_NB_stem.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1uJ2WTke_CV6",
    "outputId": "409ea2eb-11ef-440d-f918-9ef2975c5fd2"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_balanced[\"ground_truth\"], df_balanced[\"topic\"], margins=True)\n",
    "table.columns.name = None\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLo5Nh2iB6C2"
   },
   "source": [
    "# Data Split\n",
    "We split the data into 70% train, 10% validation and 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-ljBB_RCHGy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split into 80% train+val and 20% test (stratifying on both 'topic' and 'ground_truth')\n",
    "train_val_NB_lem, test_NB_lem = train_test_split(df_NB_lem, test_size=0.2, stratify=df_NB_lem[['topic', 'ground_truth']], random_state=222)\n",
    "\n",
    "# Now, split train_val into 70% train and 10% validation\n",
    "train_NB_lem, val_NB_lem = train_test_split(train_val_NB_lem, test_size=0.125, stratify=train_val_NB_lem[['topic', 'ground_truth']], random_state=222)\n",
    "# (0.125 * 80% = 10% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "yUSVxVikDCul",
    "outputId": "a85b7556-560d-4113-a0c3-f89deb63abea"
   },
   "outputs": [],
   "source": [
    "train_NB_lem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qK_igiohI0nE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# First, split into 80% train+val and 20% test (stratifying on both 'topic' and 'ground_truth')\n",
    "train_val_NB_stem, test_NB_stem = train_test_split(df_NB_stem, test_size=0.2, stratify=df_NB_stem[['topic', 'ground_truth']], random_state=222)\n",
    "\n",
    "# Now, split train_val into 70% train and 10% validation\n",
    "train_NB_stem, val_NB_stem = train_test_split(train_val_NB_stem, test_size=0.125, stratify=train_val_NB_stem[['topic', 'ground_truth']], random_state=222)\n",
    "# (0.125 * 80% = 10% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lQA1KnPYJHOM",
    "outputId": "ff82bc57-bd62-4309-96f4-42b16b36f0a0"
   },
   "outputs": [],
   "source": [
    "train_NB_stem.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GtNhRPudDGVq",
    "outputId": "ee4867d8-d7da-45ac-f98f-e01083f838db"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(train_NB_stem[\"ground_truth\"], train_NB_stem[\"topic\"], margins=True)\n",
    "table.columns.name = None\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7jY6FUnDS-x"
   },
   "outputs": [],
   "source": [
    "#table = pd.crosstab(test_NB[\"ground_truth\"], test_NB[\"topic\"], margins=True)\n",
    "#table.columns.name = None\n",
    "#table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmP3QQh_DYuq"
   },
   "outputs": [],
   "source": [
    "# First, split into 80% train+val and 20% test (stratifying on both 'topic' and 'ground_truth')\n",
    "train_val_BERT, test_BERT = train_test_split(df_bert, test_size=0.2, stratify=df_bert[['topic', 'ground_truth']], random_state=222)\n",
    "\n",
    "# Now, split train_val into 70% train and 10% validation\n",
    "train_BERT, val_BERT = train_test_split(train_val_BERT, test_size=0.125, stratify=train_val_BERT[['topic', 'ground_truth']], random_state=222)\n",
    "# (0.125 * 80% = 10% of the total data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xi-xTK-zDpAl",
    "outputId": "12742f18-5f78-4b72-9993-d4dd1469782f"
   },
   "outputs": [],
   "source": [
    "train_BERT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EHHFsX4kDv3r",
    "outputId": "39d8571a-14df-4b2b-bb7b-b2c1bbccbf7a"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(train_BERT[\"ground_truth\"], train_BERT[\"topic\"], margins=True)\n",
    "table.columns.name = None\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7CmfDqsN1na"
   },
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2L12HuQOMzg",
    "outputId": "00290b9b-aa06-4d03-ca6f-825b9026a1fe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vjJf8HiOMkF"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/BA THESIS/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn9TyBP0D8k9"
   },
   "outputs": [],
   "source": [
    "# Save the data\n",
    "train_NB_lem.to_csv(os.path.join(path, 'train_NB_lem.csv'), index=False)\n",
    "val_NB_lem.to_csv(os.path.join(path, 'val_NB_lem.csv'), index=False)\n",
    "test_NB_lem.to_csv(os.path.join(path, 'test_NB_lem.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwCQXiRQMTBa"
   },
   "outputs": [],
   "source": [
    "train_NB_stem.to_csv(os.path.join(path, 'train_NB_stem.csv'), index=False)\n",
    "val_NB_stem.to_csv(os.path.join(path, 'val_NB_stem.csv'), index=False)\n",
    "test_NB_stem.to_csv(os.path.join(path, 'test_NB_stem.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWLvKJQ0ETDt"
   },
   "outputs": [],
   "source": [
    "train_BERT.to_csv(os.path.join(path, 'train_BERT.csv'), index=False)\n",
    "val_BERT.to_csv(os.path.join(path, 'val_BERT.csv'), index=False)\n",
    "test_BERT.to_csv(os.path.join(path, 'test_BERT.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
